# ============================================================================
# LOGSTASH MAIN PIPELINE
# ============================================================================
# Pipeline: Kafka -> Processing -> Elasticsearch
# ============================================================================

input {
  # ==========================================================================
  # KAFKA INPUT
  # ==========================================================================
  kafka {
    bootstrap_servers => "kafka01:9092,kafka02:9092,kafka03:9092"
    topics => ["logs", "metrics", "events"]
    group_id => "logstash-consumer-group"
    client_id => "logstash-01"

    # Consumer settings
    auto_offset_reset => "latest"
    consumer_threads => 3
    max_poll_records => 500
    poll_timeout_ms => 1000

    # Decoding
    codec => json {
      target => "[event]"
    }

    # Metadata
    decorate_events => extended

    # Security (enable if Kafka uses SASL)
    # security_protocol => "SASL_PLAINTEXT"
    # sasl_mechanism => "PLAIN"
    # sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='logstash' password='logstash-secret';"

    # Tags
    tags => ["kafka"]
  }

  # ==========================================================================
  # BEATS INPUT (for direct Filebeat/Metricbeat ingestion)
  # ==========================================================================
  beats {
    port => 5044
    host => "0.0.0.0"
    tags => ["beats"]
  }

  # ==========================================================================
  # HTTP INPUT (for webhooks and REST APIs)
  # ==========================================================================
  http {
    port => 8080
    host => "0.0.0.0"
    codec => json
    tags => ["http"]
  }
}

filter {
  # ==========================================================================
  # COMMON PROCESSING
  # ==========================================================================

  # Add ingestion timestamp
  ruby {
    code => "event.set('@ingested_at', Time.now.utc.iso8601(3))"
  }

  # Parse Kafka metadata
  if "kafka" in [tags] {
    mutate {
      add_field => {
        "[kafka][topic]" => "%{[@metadata][kafka][topic]}"
        "[kafka][partition]" => "%{[@metadata][kafka][partition]}"
        "[kafka][offset]" => "%{[@metadata][kafka][offset]}"
        "[kafka][timestamp]" => "%{[@metadata][kafka][timestamp]}"
      }
    }
  }

  # ==========================================================================
  # LOG TYPE ROUTING
  # ==========================================================================

  # Parse JSON events if not already parsed
  if [event] {
    # Extract common fields
    if [event][message] {
      mutate {
        rename => { "[event][message]" => "message" }
      }
    }

    if [event][level] {
      mutate {
        rename => { "[event][level]" => "log_level" }
      }
    }

    if [event][service] {
      mutate {
        rename => { "[event][service]" => "[service][name]" }
      }
    }

    if [event][host] {
      mutate {
        rename => { "[event][host]" => "[host][name]" }
      }
    }

    if [event][timestamp] {
      date {
        match => [ "[event][timestamp]", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ" ]
        target => "@timestamp"
      }
      mutate {
        remove_field => [ "[event][timestamp]" ]
      }
    }
  }

  # ==========================================================================
  # LOG LEVEL NORMALIZATION
  # ==========================================================================
  if [log_level] {
    mutate {
      uppercase => [ "log_level" ]
    }

    # Map to standard levels
    translate {
      field => "log_level"
      destination => "[log][level]"
      dictionary => {
        "TRACE" => "trace"
        "DEBUG" => "debug"
        "INFO" => "info"
        "WARN" => "warn"
        "WARNING" => "warn"
        "ERROR" => "error"
        "ERR" => "error"
        "FATAL" => "fatal"
        "CRITICAL" => "fatal"
        "CRIT" => "fatal"
      }
      fallback => "info"
    }
  }

  # ==========================================================================
  # GEO IP ENRICHMENT (if IP field exists)
  # ==========================================================================
  if [source][ip] {
    geoip {
      source => "[source][ip]"
      target => "[source][geo]"
      database => "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-filter-geoip-7.2.13-java/vendor/GeoLite2-City.mmdb"
      fields => ["city_name", "country_name", "country_code2", "continent_code", "region_name", "location"]
    }
  }

  # ==========================================================================
  # USER AGENT PARSING (if user_agent field exists)
  # ==========================================================================
  if [user_agent][original] {
    useragent {
      source => "[user_agent][original]"
      target => "[user_agent]"
    }
  }

  # ==========================================================================
  # CLEANUP
  # ==========================================================================
  mutate {
    # Remove unnecessary metadata
    remove_field => [ "[event]", "@version" ]

    # Add processing pipeline info
    add_field => {
      "[logstash][pipeline]" => "main"
      "[logstash][node]" => "logstash-01"
    }
  }

  # ==========================================================================
  # DROP HEALTH CHECK LOGS
  # ==========================================================================
  if [message] =~ /health.?check|healthcheck|ping/ {
    drop { }
  }
}

output {
  # ==========================================================================
  # ELASTICSEARCH OUTPUT
  # ==========================================================================
  elasticsearch {
    # Use only es01 and es02 as primary hosts - plugin will discover other nodes
    hosts => ["https://es01:9200", "https://es02:9200"]
    user => "${ES_USER:elastic}"
    password => "${ES_PASSWORD:changeme}"
    ssl_enabled => true
    ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca/ca.crt"]
    ssl_verification_mode => "none"

    # Index naming
    index => "logs-%{+YYYY.MM.dd}"

    # ILM
    ilm_enabled => true
    ilm_rollover_alias => "logs"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "logs-policy"

    # Performance
    manage_template => true
    template_name => "logs"
    template_overwrite => true

    # Retries - increase for resilience
    retry_max_interval => 60
    retry_initial_interval => 2

    # Sniffing disabled to prevent discovery of unhealthy nodes
    sniffing => false
  }

  # ==========================================================================
  # METRICS OUTPUT (for pipeline statistics)
  # ==========================================================================
  # Uncomment to send pipeline metrics to a separate index
  # if [type] == "metrics" {
  #   elasticsearch {
  #     hosts => ["https://es01:9200"]
  #     user => "${ES_USER:elastic}"
  #     password => "${ES_PASSWORD:changeme}"
  #     ssl => true
  #     cacert => "/usr/share/logstash/config/certs/ca/ca.crt"
  #     index => "metrics-%{+YYYY.MM.dd}"
  #   }
  # }

  # ==========================================================================
  # DEBUG OUTPUT (uncomment for troubleshooting)
  # ==========================================================================
  # stdout {
  #   codec => rubydebug
  # }
}
