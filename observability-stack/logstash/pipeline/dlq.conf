# ============================================================================
# LOGSTASH DEAD LETTER QUEUE PIPELINE
# ============================================================================
# Handles failed events and routes them to DLQ topics/indices
# ============================================================================

input {
  # ==========================================================================
  # DLQ KAFKA INPUT - Reprocess failed events
  # ==========================================================================
  kafka {
    bootstrap_servers => "kafka01:9092,kafka02:9092,kafka03:9092"
    topics => ["dlq-logs", "dlq-metrics", "dlq-events", "dlq-elasticsearch-sink", "dlq-connect"]
    group_id => "logstash-dlq-consumer-group"
    client_id => "logstash-dlq-01"

    auto_offset_reset => "earliest"
    consumer_threads => 2
    max_poll_records => 100
    poll_timeout_ms => 5000

    codec => json
    decorate_events => extended

    tags => ["dlq", "reprocess"]
  }

  # ==========================================================================
  # DEAD LETTER QUEUE INPUT - From Logstash internal DLQ
  # ==========================================================================
  dead_letter_queue {
    path => "/usr/share/logstash/data/dead_letter_queue"
    commit_offsets => true
    pipeline_id => "main"
    tags => ["internal-dlq"]
  }
}

filter {
  # ==========================================================================
  # DLQ METADATA EXTRACTION
  # ==========================================================================

  # Extract DLQ metadata from Kafka headers
  if "dlq" in [tags] {
    mutate {
      add_field => {
        "[dlq][source_topic]" => "%{[@metadata][kafka][topic]}"
        "[dlq][reprocess_time]" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
        "[dlq][pipeline]" => "dlq-reprocess"
      }
    }

    # Extract error context from Connect DLQ headers
    if [__connect.errors.topic] {
      mutate {
        add_field => {
          "[dlq][original_topic]" => "%{[__connect.errors.topic]}"
          "[dlq][error_class]" => "%{[__connect.errors.exception.class.name]}"
          "[dlq][error_message]" => "%{[__connect.errors.exception.message]}"
        }
        remove_field => [
          "__connect.errors.topic",
          "__connect.errors.partition",
          "__connect.errors.offset",
          "__connect.errors.connector.name",
          "__connect.errors.task.id",
          "__connect.errors.stage",
          "__connect.errors.class.name",
          "__connect.errors.exception.class.name",
          "__connect.errors.exception.message",
          "__connect.errors.exception.stacktrace"
        ]
      }
    }
  }

  # ==========================================================================
  # INTERNAL DLQ PROCESSING
  # ==========================================================================

  if "internal-dlq" in [tags] {
    # Extract dead letter queue metadata
    ruby {
      code => '
        if event.get("[@metadata][dead_letter_queue]")
          dlq_meta = event.get("[@metadata][dead_letter_queue]")
          event.set("[dlq][reason]", dlq_meta["reason"]) if dlq_meta["reason"]
          event.set("[dlq][plugin_type]", dlq_meta["plugin_type"]) if dlq_meta["plugin_type"]
          event.set("[dlq][plugin_id]", dlq_meta["plugin_id"]) if dlq_meta["plugin_id"]
          event.set("[dlq][entry_time]", dlq_meta["entry_time"].to_s) if dlq_meta["entry_time"]
        end
      '
    }
  }

  # ==========================================================================
  # DATA VALIDATION AND REPAIR
  # ==========================================================================

  # Try to fix common JSON issues
  if [message] and ![parsed] {
    json {
      source => "message"
      target => "parsed"
      skip_on_invalid_json => true
    }

    if [parsed] {
      mutate {
        rename => { "[parsed]" => "[original_event]" }
        add_tag => ["repaired"]
      }
    }
  }

  # Validate timestamp
  if ![@timestamp] or [@timestamp] == "" {
    ruby {
      code => 'event.set("@timestamp", Time.now.utc)'
    }
    mutate {
      add_tag => ["timestamp_repaired"]
    }
  }

  # ==========================================================================
  # CLASSIFICATION
  # ==========================================================================

  # Classify DLQ event type
  ruby {
    code => '
      tags = event.get("tags") || []
      source = event.get("[dlq][source_topic]") || "unknown"

      if source.include?("logs")
        event.set("[dlq][type]", "log")
      elsif source.include?("metrics")
        event.set("[dlq][type]", "metric")
      elsif source.include?("events")
        event.set("[dlq][type]", "event")
      elsif source.include?("connect")
        event.set("[dlq][type]", "connector")
      else
        event.set("[dlq][type]", "unknown")
      end
    '
  }

  # ==========================================================================
  # RETRY COUNTER
  # ==========================================================================

  ruby {
    code => '
      retry_count = event.get("[dlq][retry_count]") || 0
      event.set("[dlq][retry_count]", retry_count.to_i + 1)

      # Mark as unrecoverable after 3 retries
      if retry_count >= 3
        event.tag("unrecoverable")
      end
    '
  }

  # ==========================================================================
  # CLEANUP
  # ==========================================================================

  mutate {
    add_field => {
      "[logstash][pipeline]" => "dlq"
      "[logstash][processed_at]" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSSZ}"
    }
  }
}

output {
  # ==========================================================================
  # RETRY TO ELASTICSEARCH (if repaired)
  # ==========================================================================

  if "repaired" in [tags] and "unrecoverable" not in [tags] {
    elasticsearch {
      hosts => ["https://es01:9200", "https://es02:9200"]
      user => "${ES_USER:elastic}"
      password => "${ES_PASSWORD:changeme}"
      ssl_enabled => true
      ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca/ca.crt"]
      ssl_verification_mode => "none"

      index => "logs-dlq-recovered-%{+YYYY.MM.dd}"

      retry_max_interval => 60
      retry_initial_interval => 5
    }
  }

  # ==========================================================================
  # ARCHIVE UNRECOVERABLE EVENTS
  # ==========================================================================

  if "unrecoverable" in [tags] {
    elasticsearch {
      hosts => ["https://es01:9200", "https://es02:9200"]
      user => "${ES_USER:elastic}"
      password => "${ES_PASSWORD:changeme}"
      ssl_enabled => true
      ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca/ca.crt"]
      ssl_verification_mode => "none"

      index => "logs-dlq-unrecoverable-%{+YYYY.MM.dd}"

      retry_max_interval => 60
      retry_initial_interval => 5
    }
  }

  # ==========================================================================
  # ALL DLQ EVENTS TO DEDICATED INDEX (for monitoring)
  # ==========================================================================

  elasticsearch {
    hosts => ["https://es01:9200", "https://es02:9200"]
    user => "${ES_USER:elastic}"
    password => "${ES_PASSWORD:changeme}"
    ssl_enabled => true
    ssl_certificate_authorities => ["/usr/share/logstash/config/certs/ca/ca.crt"]
    ssl_verification_mode => "none"

    index => "dlq-events-%{+YYYY.MM.dd}"

    retry_max_interval => 60
    retry_initial_interval => 5
  }

  # ==========================================================================
  # DEBUG OUTPUT (uncomment for troubleshooting)
  # ==========================================================================
  # stdout {
  #   codec => rubydebug
  # }
}
