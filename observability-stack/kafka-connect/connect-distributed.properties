# ============================================================================
# KAFKA CONNECT DISTRIBUTED MODE CONFIGURATION
# ============================================================================

# Kafka cluster connection
bootstrap.servers=kafka01:9092,kafka02:9092,kafka03:9092

# Unique name for the Connect cluster
group.id=connect-cluster

# Topics for storing connector and task configurations, offsets, and status
config.storage.topic=connect-configs
offset.storage.topic=connect-offsets
status.storage.topic=connect-status

# Replication factor for internal topics
config.storage.replication.factor=3
offset.storage.replication.factor=3
status.storage.replication.factor=3

# Number of partitions for offset storage topic
offset.storage.partitions=25

# Converters
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://schema-registry:8081
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://schema-registry:8081

# Internal converter (for config and offsets topics)
internal.key.converter=org.apache.kafka.connect.json.JsonConverter
internal.value.converter=org.apache.kafka.connect.json.JsonConverter
internal.key.converter.schemas.enable=false
internal.value.converter.schemas.enable=false

# REST API settings
rest.host.name=0.0.0.0
rest.port=8083
rest.advertised.host.name=kafka-connect
rest.advertised.port=8083

# Plugin path
plugin.path=/usr/share/java,/usr/share/confluent-hub-components

# Producer settings
producer.acks=all
producer.retries=10
producer.retry.backoff.ms=500
producer.compression.type=lz4
producer.batch.size=32768
producer.linger.ms=10

# Consumer settings
consumer.auto.offset.reset=earliest
consumer.max.poll.records=500
consumer.max.poll.interval.ms=300000
consumer.session.timeout.ms=60000

# Connector task settings
offset.flush.interval.ms=10000
offset.flush.timeout.ms=5000
task.shutdown.graceful.timeout.ms=10000

# Security settings (uncomment when SASL/SSL is enabled)
# security.protocol=SASL_SSL
# sasl.mechanism=PLAIN
# sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="connect" password="connect-secret";
# ssl.truststore.location=/var/ssl/private/kafka-client.truststore.jks
# ssl.truststore.password=kafka-secret

# Producer security (uncomment when SASL/SSL is enabled)
# producer.security.protocol=SASL_SSL
# producer.sasl.mechanism=PLAIN
# producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="connect" password="connect-secret";
# producer.ssl.truststore.location=/var/ssl/private/kafka-client.truststore.jks
# producer.ssl.truststore.password=kafka-secret

# Consumer security (uncomment when SASL/SSL is enabled)
# consumer.security.protocol=SASL_SSL
# consumer.sasl.mechanism=PLAIN
# consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="connect" password="connect-secret";
# consumer.ssl.truststore.location=/var/ssl/private/kafka-client.truststore.jks
# consumer.ssl.truststore.password=kafka-secret

# Metrics
metrics.jmx.prefix=kafka.connect
metrics.num.samples=2
metrics.recording.level=INFO
metrics.sample.window.ms=30000

# Error handling - Dead Letter Queue
errors.tolerance=all
errors.deadletterqueue.topic.name=dlq-connect
errors.deadletterqueue.topic.replication.factor=3
errors.deadletterqueue.context.headers.enable=true
errors.retry.delay.max.ms=60000
errors.retry.timeout=300000
errors.log.enable=true
errors.log.include.messages=true
