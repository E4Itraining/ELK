# ============================================================================
# KAFKA TIERED STORAGE CONFIGURATION (Confluent Platform)
# ============================================================================
# Archive old log segments to S3/GCS for cost-effective long-term storage
# ============================================================================

# ============================================================================
# ENABLE TIERED STORAGE
# ============================================================================
# Enable remote log storage
confluent.tier.enable=true

# Remote storage backend
confluent.tier.backend=S3

# ============================================================================
# S3 CONFIGURATION
# ============================================================================
# S3 bucket name
confluent.tier.s3.bucket=${S3_TIERED_STORAGE_BUCKET:-observability-kafka-tiered}

# S3 region
confluent.tier.s3.region=${AWS_REGION:-us-east-1}

# S3 credentials (use IAM role in production)
confluent.tier.s3.aws.access.key.id=${AWS_ACCESS_KEY_ID:-}
confluent.tier.s3.aws.secret.access.key=${AWS_SECRET_ACCESS_KEY:-}

# S3 prefix for organizing data
confluent.tier.s3.prefix=kafka-tiered-storage

# ============================================================================
# ALTERNATIVE: GCS CONFIGURATION (Google Cloud Storage)
# ============================================================================
# Uncomment to use GCS instead of S3
# confluent.tier.backend=GCS
# confluent.tier.gcs.bucket=${GCS_TIERED_STORAGE_BUCKET:-observability-kafka-tiered}
# confluent.tier.gcs.credentials.path=/etc/kafka/gcs-credentials.json

# ============================================================================
# TIERING POLICY
# ============================================================================
# Move segments to remote storage after this time (ms)
# Default: 24 hours
confluent.tier.local.hotset.ms=86400000

# Minimum bytes to retain locally
confluent.tier.local.hotset.bytes=-1

# Maximum age for tiered data (ms) - -1 for infinite
confluent.tier.max.age.ms=-1

# ============================================================================
# FETCH CONFIGURATION
# ============================================================================
# Enable fetching from remote storage
confluent.tier.fetcher.num.threads=4

# Maximum bytes to fetch in a single request
confluent.tier.max.bytes.fetched=104857600

# Fetch cache size
confluent.tier.fetcher.cache.size=104857600

# ============================================================================
# ARCHIVER CONFIGURATION
# ============================================================================
# Number of archiver threads
confluent.tier.archiver.num.threads=2

# Batch size for archiving
confluent.tier.archiver.batch.size=10

# Interval for archiving checks (ms)
confluent.tier.archiver.interval.ms=300000

# ============================================================================
# DELETION CONFIGURATION
# ============================================================================
# Enable deletion of tiered data
confluent.tier.enable.delete=true

# Delay before deleting tiered data (ms)
confluent.tier.delete.delay.ms=3600000

# ============================================================================
# ENCRYPTION (at rest)
# ============================================================================
# Use S3 server-side encryption
confluent.tier.s3.sse.algorithm=AES256

# Or use KMS
# confluent.tier.s3.sse.algorithm=aws:kms
# confluent.tier.s3.sse.kms.key.id=${KMS_KEY_ID:-}

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
# Upload timeout (ms)
confluent.tier.s3.upload.timeout.ms=60000

# Number of upload retries
confluent.tier.s3.upload.max.retries=3

# Multipart upload threshold (bytes)
confluent.tier.s3.multipart.upload.threshold=52428800

# Part size for multipart upload (bytes)
confluent.tier.s3.multipart.upload.part.size=52428800
