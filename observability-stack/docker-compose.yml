# ============================================================================
# STACK D'OBSERVABILITE ELK COMPLETE AVEC KAFKA
# ============================================================================
# Composants:
# - Elasticsearch 8.x (3 noeuds: es01, es02, es03)
# - Kibana 8.x
# - Logstash 8.x
# - Kafka 3.6.x (3 noeuds: kafka01, kafka02, kafka03)
# - Zookeeper 3.8.x
# - Prometheus
# - Alertmanager
# - Grafana (pre-provisionne)
# - Elasticsearch Exporter
# - Kafka Exporter
# - Blackbox Exporter
# - OpenTelemetry Collector
# ============================================================================

services:
  # ==========================================================================
  # SETUP - Certificate initialization (one-shot container)
  # ==========================================================================
  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION:-8.11.0}
    container_name: setup
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
    user: "0"
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        if [ ! -f config/certs/ca.zip ]; then
          echo "Creating CA";
          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
          unzip config/certs/ca.zip -d config/certs;
        fi;
        if [ ! -f config/certs/certs.zip ]; then
          echo "Creating certs";
          echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - es01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es02\n"\
          "    dns:\n"\
          "      - es02\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es03\n"\
          "    dns:\n"\
          "      - es03\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: kibana\n"\
          "    dns:\n"\
          "      - kibana\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
          unzip config/certs/certs.zip -d config/certs;
        fi;
        echo "Setting file permissions";
        chown -R root:root config/certs;
        find . -type d -exec chmod 755 \{\} \;;
        find . -type f -exec chmod 644 \{\} \;;
        echo "Certificates ready!";
      '
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120
    networks:
      - elastic

  # ==========================================================================
  # POST-SETUP - Configure passwords after ES is available (one-shot)
  # ==========================================================================
  post-setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION:-8.11.0}
    container_name: post-setup
    depends_on:
      es01:
        condition: service_healthy
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
    user: "0"
    command: >
      bash -c '
        echo "Waiting for Elasticsearch availability";
        until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q "missing authentication credentials"; do sleep 5; done;
        echo "Setting kibana_system password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 5; done;
        echo "Post-setup complete!";
      '
    networks:
      - elastic

  # ==========================================================================
  # ELASTICSEARCH CLUSTER (3 NOEUDS)
  # ==========================================================================
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION:-8.11.0}
    container_name: es01
    depends_on:
      setup:
        condition: service_completed_successfully
    environment:
      - node.name=es01
      - cluster.name=${CLUSTER_NAME:-observability-cluster}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es02,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es01/es01.key
      - xpack.security.http.ssl.certificate=certs/es01/es01.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es01/es01.key
      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE:-basic}
      - "ES_JAVA_OPTS=-Xms${ES_HEAP_SIZE:-1g} -Xmx${ES_HEAP_SIZE:-1g}"
    mem_limit: ${ES_MEM_LIMIT:-2g}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata01:/usr/share/elasticsearch/data
    ports:
      - ${ES_PORT:-9200}:9200
      - 9300:9300
    networks:
      - elastic
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    labels:
      - "observability.component=elasticsearch"
      - "observability.role=master,data,ingest"

  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION:-8.11.0}
    container_name: es02
    depends_on:
      - es01
    environment:
      - node.name=es02
      - cluster.name=${CLUSTER_NAME:-observability-cluster}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es02/es02.key
      - xpack.security.http.ssl.certificate=certs/es02/es02.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es02/es02.key
      - xpack.security.transport.ssl.certificate=certs/es02/es02.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE:-basic}
      - "ES_JAVA_OPTS=-Xms${ES_HEAP_SIZE:-1g} -Xmx${ES_HEAP_SIZE:-1g}"
    mem_limit: ${ES_MEM_LIMIT:-2g}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata02:/usr/share/elasticsearch/data
    ports:
      - 9201:9200
      - 9301:9300
    networks:
      - elastic
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    labels:
      - "observability.component=elasticsearch"
      - "observability.role=master,data,ingest"

  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION:-8.11.0}
    container_name: es03
    depends_on:
      - es02
    environment:
      - node.name=es03
      - cluster.name=${CLUSTER_NAME:-observability-cluster}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es02
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es03/es03.key
      - xpack.security.http.ssl.certificate=certs/es03/es03.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es03/es03.key
      - xpack.security.transport.ssl.certificate=certs/es03/es03.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE:-basic}
      - "ES_JAVA_OPTS=-Xms${ES_HEAP_SIZE:-1g} -Xmx${ES_HEAP_SIZE:-1g}"
    mem_limit: ${ES_MEM_LIMIT:-2g}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - esdata03:/usr/share/elasticsearch/data
    ports:
      - 9202:9200
      - 9302:9300
    networks:
      - elastic
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    labels:
      - "observability.component=elasticsearch"
      - "observability.role=master,data,ingest"

  # ==========================================================================
  # KIBANA
  # ==========================================================================
  kibana:
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION:-8.11.0}
    container_name: kibana
    depends_on:
      es01:
        condition: service_healthy
      post-setup:
        condition: service_completed_successfully
    environment:
      - SERVERNAME=kibana
      - ELASTICSEARCH_HOSTS=https://es01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
      - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY:-c34d38b3a14956121ff2170e5030b471551370178f43e5626eec58b04a30fae2}
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY:-c34d38b3a14956121ff2170e5030b471551370178f43e5626eec58b04a30fae2}
      - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY:-c34d38b3a14956121ff2170e5030b471551370178f43e5626eec58b04a30fae2}
    mem_limit: ${KIBANA_MEM_LIMIT:-1g}
    volumes:
      - certs:/usr/share/kibana/config/certs
      - kibanadata:/usr/share/kibana/data
    ports:
      - ${KIBANA_PORT:-5601}:5601
    networks:
      - elastic
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    labels:
      - "observability.component=kibana"

  # ==========================================================================
  # ZOOKEEPER
  # ==========================================================================
  zookeeper:
    image: confluentinc/cp-zookeeper:${KAFKA_VERSION:-7.5.0}
    container_name: zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: 24
      ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT: 3
      KAFKA_OPTS: "-Dzookeeper.4lw.commands.whitelist=*"
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: zookeeper
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    ports:
      - ${ZOOKEEPER_PORT:-2181}:2181
      - 9999:9999
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep -q imok"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    labels:
      - "observability.component=zookeeper"

  # ==========================================================================
  # KAFKA CLUSTER (3 NOEUDS)
  # ==========================================================================
  kafka01:
    build:
      context: ./kafka
      dockerfile: Dockerfile
      args:
        KAFKA_VERSION: ${KAFKA_VERSION:-7.5.0}
    image: kafka-with-jmx:${KAFKA_VERSION:-7.5.0}
    container_name: kafka01
    hostname: kafka01
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka01:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka01
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka01 -Dcom.sun.management.jmxremote.rmi.port=9101"
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent.jar=7071:/usr/share/jmx_exporter/jmx-exporter-config.yml"
      KAFKA_HEAP_OPTS: "-Xms${KAFKA_HEAP_SIZE:-512m} -Xmx${KAFKA_HEAP_SIZE:-512m}"
    mem_limit: ${KAFKA_MEM_LIMIT:-1g}
    volumes:
      - kafka01_data:/var/lib/kafka/data
      - ./kafka/jmx-exporter-config.yml:/usr/share/jmx_exporter/jmx-exporter-config.yml:ro
    ports:
      - 29092:29092
      - 9101:9101
      - 7071:7071
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      # Use nc (netcat) for a fast, non-blocking TCP port check.
      # This avoids blocking issues with kafka-broker-api-versions command.
      test: ["CMD-SHELL", "nc -z localhost 9092"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s
    labels:
      - "observability.component=kafka"
      - "observability.kafka.broker_id=1"

  kafka02:
    build:
      context: ./kafka
      dockerfile: Dockerfile
      args:
        KAFKA_VERSION: ${KAFKA_VERSION:-7.5.0}
    image: kafka-with-jmx:${KAFKA_VERSION:-7.5.0}
    container_name: kafka02
    hostname: kafka02
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka02:9092,PLAINTEXT_HOST://localhost:29093
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29093
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka02
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka02 -Dcom.sun.management.jmxremote.rmi.port=9101"
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent.jar=7071:/usr/share/jmx_exporter/jmx-exporter-config.yml"
      KAFKA_HEAP_OPTS: "-Xms${KAFKA_HEAP_SIZE:-512m} -Xmx${KAFKA_HEAP_SIZE:-512m}"
    mem_limit: ${KAFKA_MEM_LIMIT:-1g}
    volumes:
      - kafka02_data:/var/lib/kafka/data
      - ./kafka/jmx-exporter-config.yml:/usr/share/jmx_exporter/jmx-exporter-config.yml:ro
    ports:
      - 29093:29093
      - 9102:9101
      - 7072:7071
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      # Use nc (netcat) for a fast, non-blocking TCP port check.
      # This avoids blocking issues with kafka-broker-api-versions command.
      test: ["CMD-SHELL", "nc -z localhost 9092"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s
    labels:
      - "observability.component=kafka"
      - "observability.kafka.broker_id=2"

  kafka03:
    build:
      context: ./kafka
      dockerfile: Dockerfile
      args:
        KAFKA_VERSION: ${KAFKA_VERSION:-7.5.0}
    image: kafka-with-jmx:${KAFKA_VERSION:-7.5.0}
    container_name: kafka03
    hostname: kafka03
    depends_on:
      zookeeper:
        condition: service_healthy
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka03:9092,PLAINTEXT_HOST://localhost:29094
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29094
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_MIN_INSYNC_REPLICAS: 2
      KAFKA_NUM_PARTITIONS: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 1073741824
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_DELETE_TOPIC_ENABLE: "true"
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka03
      KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka03 -Dcom.sun.management.jmxremote.rmi.port=9101"
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent.jar=7071:/usr/share/jmx_exporter/jmx-exporter-config.yml"
      KAFKA_HEAP_OPTS: "-Xms${KAFKA_HEAP_SIZE:-512m} -Xmx${KAFKA_HEAP_SIZE:-512m}"
    mem_limit: ${KAFKA_MEM_LIMIT:-1g}
    volumes:
      - kafka03_data:/var/lib/kafka/data
      - ./kafka/jmx-exporter-config.yml:/usr/share/jmx_exporter/jmx-exporter-config.yml:ro
    ports:
      - 29094:29094
      - 9103:9101
      - 7073:7071
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      # Use nc (netcat) for a fast, non-blocking TCP port check.
      # This avoids blocking issues with kafka-broker-api-versions command.
      test: ["CMD-SHELL", "nc -z localhost 9092"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s
    labels:
      - "observability.component=kafka"
      - "observability.kafka.broker_id=3"

  # ==========================================================================
  # KAFKA EXPORTER (Consumer Lag & Topic Metrics)
  # ==========================================================================
  kafka-exporter:
    image: danielqsj/kafka-exporter:${KAFKA_EXPORTER_VERSION:-v1.7.0}
    container_name: kafka-exporter
    depends_on:
      kafka01:
        condition: service_healthy
    command:
      - '--kafka.server=kafka01:9092'
      - '--kafka.server=kafka02:9092'
      - '--kafka.server=kafka03:9092'
      - '--topic.filter=.*'
      - '--group.filter=.*'
      - '--log.level=info'
      - '--web.listen-address=:9308'
      - '--web.telemetry-path=/metrics'
      - '--concurrent.enable=true'
      - '--topic.workers=4'
      - '--kafka.version=3.6.0'
    ports:
      - 9308:9308
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:9308/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - "observability.component=kafka-exporter"

  # ==========================================================================
  # LOGSTASH
  # ==========================================================================
  logstash:
    image: docker.elastic.co/logstash/logstash:${STACK_VERSION:-8.11.0}
    container_name: logstash
    depends_on:
      setup:
        condition: service_completed_successfully
      es01:
        condition: service_healthy
      kafka01:
        condition: service_healthy
    # Wait for certificates to be available before starting Logstash
    command: >
      bash -c '
        echo "Waiting for SSL certificates...";
        while [ ! -f /usr/share/logstash/config/certs/ca/ca.crt ]; do
          echo "Certificates not ready, waiting...";
          sleep 2;
        done;
        echo "Certificates found, starting Logstash...";
        exec /usr/local/bin/docker-entrypoint
      '
    environment:
      - LS_JAVA_OPTS=-Xms${LOGSTASH_HEAP_SIZE:-512m} -Xmx${LOGSTASH_HEAP_SIZE:-512m}
      - ES_USER=elastic
      - ES_PASSWORD=${ELASTIC_PASSWORD}
      # Note: PIPELINE_WORKERS, PIPELINE_BATCH_SIZE, and LOG_LEVEL are configured
      # in logstash.yml. Setting them as env vars would trigger env2yaml to write
      # to the config file, which fails when mounted read-only.
    mem_limit: ${LOGSTASH_MEM_LIMIT:-1g}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./logstash/config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - certs:/usr/share/logstash/config/certs:ro
      - logstash_data:/usr/share/logstash/data
    ports:
      - ${LOGSTASH_PORT:-5044}:5044
      - 8080:8080
      - 9600:9600
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      # Check if Logstash API is responding (more tolerant check)
      test: ["CMD-SHELL", "curl -sf http://localhost:9600/ || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 10
      start_period: 180s
    labels:
      - "observability.component=logstash"

  # ==========================================================================
  # LOGSTASH EXPORTER (Prometheus metrics from Logstash)
  # ==========================================================================
  logstash-exporter:
    image: kuskoman/logstash-exporter:${LOGSTASH_EXPORTER_VERSION:-v1.7.1}
    container_name: logstash-exporter
    depends_on:
      logstash:
        condition: service_healthy
    environment:
      - LOGSTASH_URL=http://logstash:9600
    ports:
      - 9198:9198
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:9198/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    labels:
      - "observability.component=logstash-exporter"

  # ==========================================================================
  # PROMETHEUS
  # ==========================================================================
  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION:-v2.47.0}
    container_name: prometheus
    depends_on:
      - elasticsearch-exporter
      - blackbox-exporter
      - kafka-exporter
      - logstash-exporter
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION:-15d}'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules/:/etc/prometheus/rules/:ro
      - prometheus_data:/prometheus
    ports:
      - ${PROMETHEUS_PORT:-9090}:9090
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      start_period: 30s
      retries: 30
    labels:
      - "observability.component=prometheus"

  # ==========================================================================
  # ALERTMANAGER
  # ==========================================================================
  alertmanager:
    image: prom/alertmanager:${ALERTMANAGER_VERSION:-v0.27.0}
    container_name: alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--cluster.listen-address='
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - ${ALERTMANAGER_PORT:-9093}:9093
    networks:
      - elastic
    restart: unless-stopped
    labels:
      - "observability.component=alertmanager"

  # ==========================================================================
  # GRAFANA
  # ==========================================================================
  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-10.2.0}
    container_name: grafana
    depends_on:
      setup:
        condition: service_completed_successfully
      prometheus:
        condition: service_healthy
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=http://localhost:3000
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel
      - GF_FEATURE_TOGGLES_ENABLE=publicDashboards
      # Elasticsearch datasource credentials
      - ES_HOSTS=https://es01:9200
      - ES_USERNAME=elastic
      - ES_PASSWORD=${ELASTIC_PASSWORD}
    volumes:
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana_data:/var/lib/grafana
      - certs:/etc/grafana/certs:ro
    ports:
      - ${GRAFANA_PORT:-3000}:3000
    networks:
      - elastic
    restart: unless-stopped
    labels:
      - "observability.component=grafana"

  # ==========================================================================
  # ELASTICSEARCH EXPORTER
  # ==========================================================================
  elasticsearch-exporter:
    image: quay.io/prometheuscommunity/elasticsearch-exporter:${ES_EXPORTER_VERSION:-v1.6.0}
    container_name: elasticsearch-exporter
    depends_on:
      es01:
        condition: service_healthy
    environment:
      - ES_URI=https://elastic:${ELASTIC_PASSWORD:-changeme}@es01:9200
    # Use shell to properly expand the ES_URI environment variable
    entrypoint: ["/bin/sh", "-c", "/bin/elasticsearch_exporter --es.uri=$$ES_URI --es.ssl-skip-verify --es.all --es.indices --es.indices_settings --es.indices_mappings --es.shards --es.snapshots --es.cluster_settings --es.timeout=30s --web.listen-address=:9114 --web.telemetry-path=/metrics"]
    ports:
      - 9114:9114
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:9114/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "observability.component=elasticsearch-exporter"

  # ==========================================================================
  # BLACKBOX EXPORTER
  # ==========================================================================
  blackbox-exporter:
    image: prom/blackbox-exporter:${BLACKBOX_VERSION:-v0.24.0}
    container_name: blackbox-exporter
    command:
      - '--config.file=/etc/blackbox/blackbox.yml'
    volumes:
      - ./blackbox/blackbox.yml:/etc/blackbox/blackbox.yml:ro
    ports:
      - 9115:9115
    networks:
      - elastic
    restart: unless-stopped
    labels:
      - "observability.component=blackbox-exporter"

  # ==========================================================================
  # OPENTELEMETRY COLLECTOR
  # ==========================================================================
  otel-collector:
    image: otel/opentelemetry-collector-contrib:${OTEL_VERSION:-0.88.0}
    container_name: otel-collector
    command:
      - '--config=/etc/otel/otel-collector-config.yml'
    volumes:
      - ./otel/otel-collector-config.yml:/etc/otel/otel-collector-config.yml:ro
      # Mount host paths for hostmetrics receiver
      - /proc:/hostfs/proc:ro
      - /sys:/hostfs/sys:ro
      - /:/hostfs:ro
    environment:
      - HOST_PROC=/hostfs/proc
      - HOST_SYS=/hostfs/sys
      - HOST_ETC=/hostfs/etc
      - HOST_VAR=/hostfs/var
      - HOST_RUN=/hostfs/run
      - HOST_DEV=/hostfs/dev
    ports:
      - 4317:4317   # OTLP gRPC receiver
      - 4318:4318   # OTLP HTTP receiver
      - 8888:8888   # Prometheus metrics exposed by the collector
      - 8889:8889   # Prometheus exporter metrics
      - 13133:13133 # health_check extension
      - 55679:55679 # zpages extension
    networks:
      - elastic
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O /dev/null http://localhost:13133/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    labels:
      - "observability.component=otel-collector"

  # ==========================================================================
  # LOG INJECTOR
  # ==========================================================================
  log-injector:
    build:
      context: ./log-injector
      dockerfile: Dockerfile
    container_name: log-injector
    depends_on:
      setup:
        condition: service_completed_successfully
      es01:
        condition: service_healthy
    environment:
      - ES_HOST=https://es01:9200
      - ES_USER=elastic
      - ES_PASSWORD=${ELASTIC_PASSWORD}
      - ES_VERIFY_CERTS=false
      - INJECTION_RATE=${LOG_INJECTION_RATE:-10}
      - INDEX_PREFIX=${LOG_INDEX_PREFIX:-logs}
    volumes:
      - certs:/app/certs:ro
    networks:
      - elastic
    profiles:
      - inject
    labels:
      - "observability.component=log-injector"

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  elastic:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  certs:
    driver: local
  esdata01:
    driver: local
  esdata02:
    driver: local
  esdata03:
    driver: local
  kibanadata:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_log:
    driver: local
  kafka01_data:
    driver: local
  kafka02_data:
    driver: local
  kafka03_data:
    driver: local
  logstash_data:
    driver: local
  prometheus_data:
    driver: local
  alertmanager_data:
    driver: local
  grafana_data:
    driver: local
